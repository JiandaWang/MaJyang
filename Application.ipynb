{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image, ImageGrab, ImageDraw, ImageFont\n",
    "from torchvision import transforms, models\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyWinhook\n",
    "import pythoncom\n",
    "import time\n",
    "import win32api\n",
    "\n",
    "import import_ipynb\n",
    "import RequiredPai_for_import as RP\n",
    "from Separation import g_EgoHandCropLeftUpperX_int, g_EgoHandCropLeftUpperY_int, \\\n",
    "                       g_EgoHandCropRightLowerX_int, g_EgoHandCropRightLowerY_int, \\\n",
    "                       g_EgoHandCropMinPaiArea_int, g_EgoHandGaussKSize_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global constants\n",
    "\n",
    "# since the data are not \"nature\" objects, pre-calulate the mean and std for normalization\n",
    "# see the notebook NormalizationSupport\n",
    "# note: these value are not imported from NormalizationSupport, because it will re-calculte every time when you import\n",
    "g_NormMean_lst = [0.7406573632924783, 0.7232460663149365, 0.6959515512913943]\n",
    "g_NormStd_lst = [0.22704669694242244, 0.24139483459696767, 0.2690579683428062]\n",
    "\n",
    "# average size of input data; will be used for input data scalling; depending on your screen size!\n",
    "# see the notebook NormalizationSupport\n",
    "# note: these value are not imported from NormalizationSupport, because it will re-calculte every time when you import\n",
    "g_AvgW_int = 83\n",
    "g_AvgH_int = 136\n",
    "# scale factor and final input image size which fulfill the 224x224 requirement\n",
    "g_ScaleFactor_int = max(224//g_AvgW_int, 224//g_AvgH_int)+1 # in case original input size already big enough, the factor would be one\n",
    "g_UpSampledW_int = g_ScaleFactor_int*g_AvgW_int\n",
    "g_UpSampledH_int = g_ScaleFactor_int*g_AvgH_int\n",
    "\n",
    "# normal font path\n",
    "# ImageDraw will search certain default paths if you only give the font name; please refer the documentation\n",
    "g_FontPath_str = 'arial.ttf'\n",
    "\n",
    "# PAI font path; should be ttf format and support extend utf-8 characters of Mahjong tiles\n",
    "# current choise: BabelStoneHan\n",
    "g_PAIFontPath_str = r'..\\..\\..\\..\\AppData\\Local\\Microsoft\\Windows\\Fonts\\BabelStoneHan.ttf'\n",
    "\n",
    "# path for pre-trained model\n",
    "g_PreTrainedModelPath_str = r'Model\\20210317_Trial1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse dictionary to get required PAIs\n",
    "g_PAI_Reverse_Dict_Prediction_c = {0 : '1m', \\\n",
    "                                   1 : '2m', \\\n",
    "                                   2 : '3m', \\\n",
    "                                   3 : '4m', \\\n",
    "                                   4 : '5m', \\\n",
    "                                   5 : '6m', \\\n",
    "                                   6 : '7m', \\\n",
    "                                   7 : '8m', \\\n",
    "                                   8 : '9m', \\\n",
    "                                   9 : '1p', \\\n",
    "                                   10 : '2p', \\\n",
    "                                   11 : '3p', \\\n",
    "                                   12 : '4p', \\\n",
    "                                   13 : '5p', \\\n",
    "                                   14 : '6p', \\\n",
    "                                   15 : '7p', \\\n",
    "                                   16 : '8p', \\\n",
    "                                   17 : '9p', \\\n",
    "                                   18 : '1s', \\\n",
    "                                   19 : '2s', \\\n",
    "                                   20 : '3s', \\\n",
    "                                   21 : '4s', \\\n",
    "                                   22 : '5s', \\\n",
    "                                   23 : '6s', \\\n",
    "                                   24 : '7s', \\\n",
    "                                   25 : '8s', \\\n",
    "                                   26 : '9s', \\\n",
    "                                   27 : 'E', \\\n",
    "                                   28 : 'S', \\\n",
    "                                   29 : 'W', \\\n",
    "                                   30 : 'N', \\\n",
    "                                   31 : 'R', \\\n",
    "                                   32 : 'G', \\\n",
    "                                   33 : 'Wh', \\\n",
    "                                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyLoader(f_ImgPointer_str):\n",
    "    global g_ScaleFactor_int\n",
    "    global g_UpSampledW_int\n",
    "    global g_UpSampledH_int\n",
    "    \n",
    "    # open the picture with PIL\n",
    "    # upsample\n",
    "    # rescale it to [0, 1] and convert to tensor\n",
    "    # center crop\n",
    "    # normalize\n",
    "    \n",
    "    # \"open the picture\"\n",
    "    l_im_img = f_ImgPointer_str\n",
    "    \n",
    "    # upsample\n",
    "    if g_ScaleFactor_int > 1: \n",
    "        l_im_img = l_im_img.resize((l_im_img.size[0]*g_ScaleFactor_int, l_im_img.size[1]*g_ScaleFactor_int), resample=Image.NEAREST)\n",
    "    \n",
    "    # to tensor (implicated also rescale to [0, 1])\n",
    "    # note: ToTensor is a class!\n",
    "    l_im_tsr = transforms.ToTensor()(l_im_img)\n",
    "\n",
    "    # normalize; note: also a class!\n",
    "    l_im_tsr = transforms.Normalize(g_NormMean_lst, g_NormStd_lst)(l_im_tsr)\n",
    "    \n",
    "    # Center Crop\n",
    "    l_im_tsr = transforms.CenterCrop((g_UpSampledH_int, g_UpSampledW_int))(l_im_tsr)\n",
    "\n",
    "    # return\n",
    "    return l_im_tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgpointers, loader=MyLoader, transform=None):\n",
    "        self.images = imgpointers\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgpointer = self.images[index]\n",
    "        img = self.loader(imgpointer)\n",
    "        if self.transform: img = self.transform(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeparationEgoHand(f_ScreenShot_img):\n",
    "    global g_EgoHandOutputFolderPath_str\n",
    "    global g_EgoHandCropLeftUpperX_int\n",
    "    global g_EgoHandCropLeftUpperY_int\n",
    "    global g_EgoHandCropRightLowerX_int\n",
    "    global g_EgoHandCropRightLowerY_int\n",
    "    global g_EgoHandCropMinPaiArea_int\n",
    "    global g_EgoHandGaussKSize_int\n",
    "    \n",
    "    # \"open image\"\n",
    "    l_im_img = f_ScreenShot_img\n",
    "    \n",
    "    # crop the interesting part\n",
    "    l_im_img = l_im_img.crop((g_EgoHandCropLeftUpperX_int, g_EgoHandCropLeftUpperY_int, \\\n",
    "                              g_EgoHandCropRightLowerX_int, g_EgoHandCropRightLowerY_int))\n",
    "    \n",
    "    # convert into 8 bit gray scale image for further operation\n",
    "    l_im_gs_img = l_im_img.convert('L')\n",
    "    l_im_gs_npa = np.array(l_im_gs_img) # convert to np array for cv2\n",
    "    \n",
    "    # originally from game screenshot, therefor gaussian noise filter not necessary\n",
    "    # still use the gaussian for better contour detection\n",
    "    l_im_gs_npa = cv2.GaussianBlur(l_im_gs_npa, (g_EgoHandGaussKSize_int, g_EgoHandGaussKSize_int), 0)\n",
    "    \n",
    "    # detect edges with canny\n",
    "    l_im_edge_npa = cv2.Canny(l_im_gs_npa, 30, 100)\n",
    "    \n",
    "    # detect counters based on edges\n",
    "    # only outside edges and simple 3 directions for edges\n",
    "    l_contour, _ = cv2.findContours(l_im_edge_npa, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "    \n",
    "    # find rectangles for each contour\n",
    "    # initialize output list\n",
    "    l_OutputImages_lst = []\n",
    "    l_AnchorCoords_lst = []\n",
    "    for c in l_contour:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        \n",
    "        # in case rectangles big enough\n",
    "        if w*h >= g_EgoHandCropMinPaiArea_int:\n",
    "            # get the correspoding area from original image\n",
    "            l_im_Pai_img = l_im_img.crop((x, y, x+w, y+h))\n",
    "            # append to output list\n",
    "            l_OutputImages_lst.append(l_im_Pai_img)\n",
    "            # append the anchor coordinates in original\n",
    "            l_AnchorCoords_lst.append([x+g_EgoHandCropLeftUpperX_int,y+g_EgoHandCropLeftUpperY_int, w, h])\n",
    "\n",
    "    # return\n",
    "    return l_OutputImages_lst, l_AnchorCoords_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawPrediction(f_im_img, f_AnchorCoords_lst, f_PredictedPAI_lst, f_NecessaryPAI_lst):\n",
    "    # find rectangles for each contour\n",
    "    # get the drawer\n",
    "    l_Drawer = ImageDraw.Draw(f_im_img)\n",
    "    # get the font\n",
    "    font = ImageFont.truetype(g_FontPath_str, size=30)\n",
    "\n",
    "    # loop over all elements\n",
    "    for i in range(len(f_PredictedPAI_lst)):\n",
    "        x = f_AnchorCoords_lst[i][0]\n",
    "        y = f_AnchorCoords_lst[i][1]\n",
    "        w = f_AnchorCoords_lst[i][2]\n",
    "        h = f_AnchorCoords_lst[i][3]\n",
    "        \n",
    "        # draw rectangle on the separated area\n",
    "        l_Drawer.rectangle([x, y, x+w, y+h], outline='red', width=3)\n",
    "        l_Drawer.text((x+w/2, y-5), f_PredictedPAI_lst[i], fill='red', font=font, anchor='mb')\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # get the font\n",
    "    l_PAI_FontSize_int = 40\n",
    "    font = ImageFont.truetype(g_PAIFontPath_str, size=l_PAI_FontSize_int)\n",
    "    \n",
    "    # generate the lines and decide the width of the background box\n",
    "    l_BGWidth_int = 0\n",
    "    l_Lines_lst = []\n",
    "    for NecPAIs in f_NecessaryPAI_lst:\n",
    "        text = '打'\n",
    "        text += RP.g_PAI_Disp_c[NecPAIs[0]]\n",
    "        text += '摸'\n",
    "        for PAI in NecPAIs[2]: text += RP.g_PAI_Disp_c[PAI]\n",
    "        text += str(NecPAIs[3])\n",
    "        text += '枚'\n",
    "        l_Lines_lst.append(text)\n",
    "        l_BGWidth_int = max(l_BGWidth_int, l_Drawer.textsize(text, font=font)[0])\n",
    "\n",
    "    # set the anchor\n",
    "    l_BGWidth_int += 10\n",
    "    l_BGHeight_int = len(f_NecessaryPAI_lst) * (l_PAI_FontSize_int + 5)\n",
    "    l_AnchorX_int = f_im_img.size[0]/2-l_BGWidth_int/2\n",
    "    l_AnchorY_int = f_im_img.size[1]/2-l_BGHeight_int/2\n",
    "    l_Drawer.rectangle([l_AnchorX_int, l_AnchorY_int, l_AnchorX_int+l_BGWidth_int, l_AnchorY_int+l_BGHeight_int], fill='white')   \n",
    "    \n",
    "    # move anchor\n",
    "    l_AnchorX_int += 5\n",
    "    l_AnchorY_int += 5\n",
    "    # print background for required PAIs information\n",
    "    for text in l_Lines_lst:\n",
    "        l_Drawer.text((l_AnchorX_int, l_AnchorY_int), text, fill='black', font=font, anchor='lt')\n",
    "        # move anchor\n",
    "        l_AnchorY_int += l_PAI_FontSize_int + 5\n",
    "    \n",
    "    # return\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalNecessaryPAIsBasedOnPredictedHand(f_Hand_lst):\n",
    "    # get the input hand sorted\n",
    "    hand = sorted(f_Hand_lst, key=lambda PAI: RP.g_PAI_Dict_c[PAI])\n",
    "    \n",
    "    # loop over all PAIs, try to play it and calcualte the number of necessary PAIs / required PAIs\n",
    "    # initialize variable(s)\n",
    "    min_Num_Nec = RP.g_invalid_Num_Of_Necessary_PAIs_c\n",
    "    list_Num_Nec_Required_PAIs = [] # played PAI, number of necessary PAIs, required PAIs, number of required PAIs\n",
    "    # loop once over all PAIs in hand to remove them from the current pool\n",
    "    l_CurPAINum_dict = RP.g_PAI_Num_c.copy()\n",
    "    for PAI in hand: l_CurPAINum_dict[PAI] -= 1    \n",
    "    # loop\n",
    "    for played_PAI in hand:\n",
    "        # only check not duplicated PAIs\n",
    "        if list_Num_Nec_Required_PAIs == [] \\\n",
    "        or not played_PAI == list_Num_Nec_Required_PAIs[-1][0]:\n",
    "            # remove the played PAI from hand\n",
    "            temp_hand = hand.copy(); temp_hand.remove(played_PAI)\n",
    "            # calculate the number of necessary PAIs\n",
    "            temp_required_PAIs = set()\n",
    "            temp_num_Nec = RP.CalNumOfNecessaryPAIs(temp_hand, temp_required_PAIs)\n",
    "            temp_required_PAIs = sorted(temp_required_PAIs, key=lambda PAI: RP.g_PAI_Dict_c[PAI])\n",
    "            # add the output into list\n",
    "            min_Num_Nec = min(min_Num_Nec, temp_num_Nec)\n",
    "            list_Num_Nec_Required_PAIs.append([played_PAI, temp_num_Nec, temp_required_PAIs, \\\n",
    "                                               RP.GetRemainingNumOfRequiredPAIs(l_CurPAINum_dict, temp_required_PAIs)])\n",
    "\n",
    "    # sort the output list by number of required PAIs\n",
    "    list_Num_Nec_Required_PAIs = sorted(list_Num_Nec_Required_PAIs, key=lambda out_list: out_list[3], reverse=True)\n",
    "    # remove all options which can not provide the mininal number of necessary PAIs\n",
    "    index = 0\n",
    "    while index < len(list_Num_Nec_Required_PAIs):\n",
    "        if list_Num_Nec_Required_PAIs[index][1] == min_Num_Nec:\n",
    "            index += 1\n",
    "        else:\n",
    "            list_Num_Nec_Required_PAIs.pop(index)\n",
    "    \n",
    "    # return the list of necessary PAIs\n",
    "    return list_Num_Nec_Required_PAIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onKeyboardEvent(f_event):\n",
    "    global g_hm\n",
    "    global g_SaveImagePath_str\n",
    "    global g_Net\n",
    "\n",
    "    # when 'C' is pressed, save screen shot of current screen\n",
    "    # naming rule: yyyymmdd_hhmmss\n",
    "    if f_event.Key == 'C':\n",
    "        # get current screenshot\n",
    "        l_im_img = ImageGrab.grab()\n",
    "        \n",
    "        # get separated PAI list\n",
    "        l_im_PAIs_lst, l_AnchorCoords_lst = SeparationEgoHand(l_im_img)\n",
    "        \n",
    "        # get dataloader\n",
    "        l_ValidationData_DaSt = MyDataset(l_im_PAIs_lst)\n",
    "        l_ValidationData_DaLder = torch.utils.data.DataLoader(l_ValidationData_DaSt, batch_size=len(l_im_PAIs_lst), shuffle=False)\n",
    "        \n",
    "        # send throw net\n",
    "        for i, data in enumerate(l_ValidationData_DaLder, 0):\n",
    "            outputs = g_Net(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # get the PAI from index\n",
    "            l_PredictedPAI_lst = [g_PAI_Reverse_Dict_Prediction_c[x.item()] for x in predicted]\n",
    "            # calculated the required PAIs\n",
    "            l_NecessaryPAI_lst = CalNecessaryPAIsBasedOnPredictedHand(l_PredictedPAI_lst)\n",
    "            # display the PAIs on original image\n",
    "            DrawPrediction(l_im_img, l_AnchorCoords_lst, l_PredictedPAI_lst, l_NecessaryPAI_lst)\n",
    "        \n",
    "        # show the image\n",
    "        l_im_img.show()\n",
    "        \n",
    "        # release memory\n",
    "        l_im_img.close()\n",
    "        \n",
    "    # when 'Escape' is pressed, stop monitoring & hook\n",
    "    if f_event.Key == 'Escape':\n",
    "        win32api.PostQuitMessage() # send quit message\n",
    "        g_hm.UnhookKeyboard() # unhook the mouse\n",
    "        print('Hook stopped.')\n",
    "\n",
    "    # return\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and load the model\n",
    "# get the ResNet18 model as base and replace the last fully connected layer with custom output\n",
    "# after that load local pretrained parameter\n",
    "g_Net = models.resnet18(pretrained=False, progress=False)\n",
    "in_ftr  = g_Net.fc.in_features\n",
    "out_ftr = 34 # 34 different PAIs available\n",
    "g_Net.fc = torch.nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "g_Net.load_state_dict(torch.load(g_PreTrainedModelPath_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "# get hook manager\n",
    "g_hm = pyWinhook.HookManager()\n",
    "# set call back function for keyboard hook\n",
    "g_hm.KeyDown = onKeyboardEvent\n",
    "# start keyboard hook\n",
    "g_hm.HookKeyboard()\n",
    "\n",
    "# start monitering\n",
    "pythoncom.PumpMessages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-zealand",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
